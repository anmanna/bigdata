{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connect to the shared google drive"
      ],
      "metadata": {
        "id": "RwjS689mPmmF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGQ1DVwhw84G",
        "outputId": "a52fd988-e5d3-43ad-d127-bd29c0b12690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary library and packages"
      ],
      "metadata": {
        "id": "rnSDOf2dPsON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import html\n",
        "import re\n",
        "!pip install geopy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm # Use a linear svm model\n",
        "from sklearn.metrics import classification_report\n",
        "# Use gridsearch to hypertune parameters for linear svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pickle\n",
        "from nltk.tokenize.sonority_sequencing import SyllableTokenizer\n",
        "tk = SyllableTokenizer()\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPz8uVE8xbGV",
        "outputId": "c84362b5-0f2a-46dc-a26b-ccdeef97bf31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy) (1.52)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the raw data scraped from Twitter"
      ],
      "metadata": {
        "id": "Ge2Z2yRjPzKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data\n",
        "\n",
        "barbera = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Barbera.csv')\n",
        "cabernet = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Cabernet.csv')\n",
        "chardonnay = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Chardonnay.csv')\n",
        "ciu_ciu_1 = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Ciu Ciu.csv')\n",
        "ciu_ciu_2 = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Ciù Ciù.csv')\n",
        "ciu_ciu_3 = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/CiuCiu.csv')\n",
        "ciu_ciu_4 = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/CiùCiù.csv')\n",
        "merlot = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Merlot.csv',lineterminator='\\n')\n",
        "montepulciano = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Montepulciano.csv', lineterminator='\\n')\n",
        "moscato = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Moscato.csv', lineterminator='\\n')\n",
        "pinot_grigio = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Pinot Grigio.csv', lineterminator='\\n')\n",
        "sangiovese = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Sangiovese.csv', lineterminator='\\n')\n",
        "sauvignon = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Sauvignon.csv', lineterminator='\\n')\n",
        "syrah = pd.read_csv('/content/drive/Shareddrives/Big data/data_v2/Syrah.csv', lineterminator='\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJu53Z2SxoHX",
        "outputId": "dfe7edf9-89b1-4383-cd8b-796b8bbf0ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0,6,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine the 4 separate data files for Ciu Ciu into one"
      ],
      "metadata": {
        "id": "oqMEbBUDP3_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ciu_ciu = pd.concat([ciu_ciu_1,ciu_ciu_2,ciu_ciu_3,ciu_ciu_4])"
      ],
      "metadata": {
        "id": "zCn-1uuM1ZMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove unwanted columns from each data set"
      ],
      "metadata": {
        "id": "goAHvX4nQBbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unwanted columns\n",
        "\n",
        "cols = ['replyCount', 'retweetCount', 'likeCount', 'quoteCount']\n",
        "df = [barbera, cabernet, chardonnay, ciu_ciu, merlot, montepulciano, moscato, pinot_grigio, sangiovese, sauvignon, syrah ]\n",
        "for i in df:\n",
        "  i.drop(cols,axis=1,inplace=True)\n"
      ],
      "metadata": {
        "id": "B-Pk-L3D2PDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the cleaning function for the location data from the twitter data sets"
      ],
      "metadata": {
        "id": "NwRqUmLVQJMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # cleaning location data\n",
        "\n",
        "def location_cleaner(df):\n",
        "  df = df[~df['user_location'].str.contains('#',na=False)] #remove rows containing hashtags\n",
        "\n",
        "  loc_nan_values = df[df['user_location'].isna()]\n",
        "  df = df.drop(loc_nan_values.index) #identify and drop na values from location column\n",
        "\n",
        "  # removing unknown words from user_location data\n",
        "  df['extracted_user_loc'] = df['user_location'].astype(str).apply(lambda x: list(nlp(x).ents) if len(list(nlp(x).ents))>0 else np.nan)\n",
        "    \n",
        "  # identify and dropp na values from extracted user location column\n",
        "  nan_values = df[df['extracted_user_loc'].isna()]\n",
        "  df = df.drop(nan_values.index)\n",
        "\n",
        "  # sample 1/1000 data rows for geolocation\n",
        "  number_samples = int(len(df)/1000) + 2\n",
        "  df_sample = df.sample(n=number_samples,random_state=3)\n",
        "\n",
        "  # geocode the sampled data\n",
        "  geolocator = Nominatim(user_agent=\"my-application\")\n",
        "  geocode = RateLimiter(geolocator.geocode, min_delay_seconds=3,max_retries=3)\n",
        "  df_sample['location'] = df_sample['user_location'].progress_apply(geocode)\n",
        "\n",
        "  # remove any rows that contain na in the new geocoded location\n",
        "  df_nan_values = df_sample[df_sample['location'].isna()]\n",
        "  df_sample = df_sample.drop(df_nan_values.index)\n",
        "\n",
        "  return df_sample\n"
      ],
      "metadata": {
        "id": "01-j9mBp9oTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean & preprocess the location data & save each cleaned location file as a .csv to the google shared drive"
      ],
      "metadata": {
        "id": "P_chhM_0QKr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "syrah1 = location_cleaner(syrah)\n",
        "syrah1.to_csv('syrah_location.csv')\n",
        "!cp syrah_location.csv \"drive/My Drive/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc2yVTdchuZq",
        "outputId": "d05b58e9-c8fa-4a8a-9502-fbb343ef6b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [02:27<00:00,  3.28s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the preprocessing function for the text data from Twitter"
      ],
      "metadata": {
        "id": "P7SOygbPQPPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the text data\n",
        "\n",
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "def preprocessor(df):\n",
        "\n",
        "  df[\"text\"] = df[\"text\"].astype(str)\n",
        "\n",
        "  for i in range (len(df)):\n",
        "    x = df['text'][i].replace('\\n',' ') #cleaning newline “\\n” from the tweets\n",
        "    df['text'][i] = html.unescape(x)\n",
        "\n",
        "  for i in range (len(df)):\n",
        "    df['text'][i] = re.sub(r'<br /><br />|(@[A-Za-z0–9_]+)|(#[A-Za-z0–9_]+)|[^\\w\\s]|http\\S+', ' ', df['text'][i]) # add removal items for <br /><br /> and #something\n",
        "\n",
        "  df['tweets_to_token'] = df['text']\n",
        "  sw = stopwords.words('english') #you can adjust the language as you desire\n",
        "  sw.remove('not') #we exclude not from the stopwords corpus since removing not from the text will change the context of the text\n",
        "\n",
        "  for i in range(len(df['tweets_to_token'])):\n",
        "    df['tweets_to_token'][i] = word_tokenize(df['tweets_to_token'][i]) # do the word tokenize\n",
        "\n",
        "  for token in df['tweets_to_token'][i]:\n",
        "    df['tweets_to_token'][i] = tk.tokenize(token)\n",
        "    flatten(df['tweets_to_token'][i])\n",
        "\n",
        "  for i in range(len(df['tweets_to_token'])):\n",
        "    df['tweets_to_token'][i] = ' '.join([word for word in df['tweets_to_token'][i] if not word in sw]) # turn the tokenized listf into string to fit the format for applying CountVectorizer()\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "-lfrlBirZgfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean & preprocess the text data & save each cleaned data as a .csv file to the google shared drive"
      ],
      "metadata": {
        "id": "_koGaxnUQdGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "syrah_token = preprocessor(syrah)\n",
        "syrah_token.to_csv('syrah_token.csv')\n",
        "!cp syrah_token.csv \"drive/Shareddrives/Big data/data_v2/\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BBuQUa-ZuDC",
        "outputId": "7633121a-9a47-47cf-dc33-fd40680edff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '8'\n",
            "  \" assigning as vowel: '{}'\".format(c)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '4'\n",
            "  \" assigning as vowel: '{}'\".format(c)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}
