{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMltiRG4IteMrOYETRNKxJj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Data crawling"],"metadata":{"id":"QHrrzBZV5tXi"}},{"cell_type":"code","source":["import snscrape.modules.twitter as sntwitter\n","import pandas as pd\n","\n","class_lst = ['Barbera', 'Cabernet', 'Chardonnay', 'Merlot', 'Montepulciano', 'Moscato', 'Pinot Grigio', 'Sangiovese', 'Sauvignon', 'Syrah', 'CiuCiu']\n","#NUM = 10000\n","\n","for i, grape_type in enumerate(class_lst):#\n","    if i != len(class_lst)-1:\n","        continue\n","    if grape_type == 'CiuCiu':\n","        text_query = \"wine, \" + grape_type + ' lang:en'\n","    else:\n","        text_query = \"wine, \" + grape_type + ' lang:en since:2015-01-01 until:2022-09-30'\n","    result = []\n","    for n, tweet in enumerate(sntwitter.TwitterSearchScraper(text_query).get_items()):\n","        '''if n > NUM:\n","            break'''\n","        print('============================================')\n","        print(n)\n","        print(tweet.id)\n","        print(tweet.rawContent)\n","        print(tweet.date)\n","        print(tweet.user.username)\n","        print(tweet.user.location)\n","        print(tweet.replyCount)\n","        print(tweet.retweetCount)\n","        print(tweet.likeCount)\n","        print(tweet.quoteCount)\n","        result.append({ 'id':tweet.id,\n","                        'text':tweet.rawContent,\n","                        'time':tweet.date,\n","                        'user_location':tweet.user.location,\n","                        'username':tweet.user.username,\n","                        'replyCount':tweet.replyCount,\n","                        'retweetCount':tweet.retweetCount,\n","                        'likeCount':tweet.likeCount,\n","                        'quoteCount':tweet.quoteCount,\n","                        'class':grape_type\n","            })\n","\n","    df = pd.DataFrame(result)\n","    df.to_csv(grape_type+'.csv',index=False)"],"metadata":{"id":"o2wKE9ii5nDI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data Preprocess"],"metadata":{"id":"jeWw9rFA5jLI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"itrTjK9t5cVp"},"outputs":[],"source":["# import the libraries\n","import pandas as pd\n","import html\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.sonority_sequencing import SyllableTokenizer\n","tk = SyllableTokenizer()\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","def flatten(l):\n","    return [item for sublist in l for item in sublist]\n","\n","def preprocessor(df):\n","\n","  for i in range (len(df)):\n","    x = df['text'][i].replace('\\n',' ') #cleaning newline “\\n” from the tweets\n","    df['text'][i] = html.unescape(x)\n","\n","  for i in range (len(df)):\n","    df['text'][i] = re.sub(r'<br /><br />|(@[A-Za-z0–9_]+)|(#[A-Za-z0–9_]+)|[^\\w\\s]|http\\S+', ' ', df['text'][i]) # add removal items for <br /><br /> and #something\n","  \n","  tweets_to_token = df['text']\n","  sw = stopwords.words('english') #you can adjust the language as you desire\n","  sw.remove('not') #we exclude not from the stopwords corpus since removing not from the text will change the context of the text\n","\n","  return tweets_to_token\n","\n","def tokenise(tweets_to_token):\n","  for i in range(len(tweets_to_token)):\n","    tweets_to_token[i] = word_tokenize(tweets_to_token[i]) # do the word tokenize\n","\n","  for token in tweets_to_token[i]:\n","    tweets_to_token[i] = tk.tokenize(token)\n","    flatten(tweets_to_token[i])\n","\n","  for i in range(len(tweets_to_token)):\n","    tweets_to_token[i] = ' '.join([word for word in tweets_to_token[i] if not word in sw]) # turn the tokenized listf into string to fit the format for applying CountVectorizer()\n","  \n","  return tweets_to_token\n","\n","\n","# read the data\n","cols = ['sentiment','id','date','query_string','user','text']\n","df_train = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1',header=None,names=cols)\n","print('Finish read train data')\n","df_test = pd.read_csv('testdata.manual.2009.06.14.csv', encoding='latin-1',header=None,names=cols)\n","print('Finish read test data')\n","\n","# eliminated un-needed columns \n","df_train.drop(['id','date','query_string','user'],axis=1,inplace=True)\n","print('Finish drop train data')\n","df_test.drop(['id','date','query_string','user'],axis=1,inplace=True)\n","print('Finish drop test data')\n","\n","#start preprocessing\n","train_token = preprocessor(df_train)\n","train_token['sentiment'] = df_train['sentiment']#combine data with sentiment label to enable parallel without messy the data\n","train_token.to_csv('train_preprocessed.csv', index = False)\n","print('Finish preprocess train data')\n","test_token = preprocessor(df_test)\n","test_token['sentiment'] = df_test['sentiment']\n","test_token.to_csv('test_preprocessed.csv', index = False)\n","print('Finish preprocess test data')\n"]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"YLKvvjZv5-VC"}},{"cell_type":"code","source":["#Reference from: https://stackoverflow.com/questions/28396957/sklearn-tfidf-vectorizer-to-run-as-parallel-jobs\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import multiprocessing\n","from multiprocessing import Pool\n","import scipy.sparse as sp\n","from sklearn.model_selection import GridSearchCV\n","import joblib\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import svm\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import uniform\n","\n","filename = 'svm_model.pkl'\n","num_cores = multiprocessing.cpu_count()\n","num_partitions = num_cores-2 # I like to leave some cores for other\n","#processes\n","print(num_partitions)\n","def parallelize_dataframe(df, func):\n","    a = np.array_split(df, num_partitions)\n","    del df\n","    pool = Pool(num_cores)\n","    df = sp.vstack(pool.map(func, a), format='csr')\n","    pool.close()\n","    pool.join()\n","    return df\n","\n","def test_func(data):\n","    tfidf_matrix = vectorizer.transform(data[\"text\"])\n","    return tfidf_matrix\n","\n","# read the preprocess data\n","train_token = pd.read_csv('train_preprocessed.csv')\n","#train_token = train_token.join(df_train['sentiment'])\n","#print(train_token.columns)\n","#train_token.to_csv('train_preprocessed.csv', index=False)\n","print('Finish reading preprocessed train data')\n","test_token = pd.read_csv('test_preprocessed.csv')\n","test_token = test_token[test_token.sentiment != 2.0]# drop the class 2, since there is no class 2 in the testing dataset, so we won't get class 2 however hard we tried. Drop it to get the better result for test. \n","#test_token = test_token.join(df_test['sentiment'])\n","#test_token.to_csv('test_preprocessed.csv', index=False)\n","print('Finish reading preprocessed test data')\n","#print(train_token.head(4))\n","'''\n","# let the vectorizer fit the data\n","vectorizer = TfidfVectorizer()#max_features=10000\n","X_train = vectorizer.fit(train_token['text'].values)\n","with open('vectorizer.pkl', 'wb') as file:\n","    pickle.dump(vectorizer, file)\n","print('Finish vectorize train data')\n","'''\n","#vectorize the data\n","with open('vectorizer.pkl', 'rb') as file:\n","  vectorizer = pickle.load(file)\n","X_test = vectorizer.transform(test_token['text'].values).toarray()\n","print('Finish vectorize test data')\n","#X_train = vectorizer.transform(train_token['text'].values.tolist()).toarray()\n","X_train = parallelize_dataframe(train_token, test_func)\n","print('Finish vectorize save')\n","\n","#start to do grid search\n","#################### MultinomialNB #######################\n","'''param={'alpha': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000], 'fit_prior': [True, False]}\n","pre_model = MultinomialNB() #svm paramters tuning\n","model = GridSearchCV(pre_model,\n","                      param_grid=param,\n","                      scoring='accuracy',\n","                      cv=5,\n","                      return_train_score=True)'''\n","##################################################################\n","#################### svm #######################\n","'''param={'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'], \n","    'degree': [1,3,5,7,9], \n","    'gamma': ['scale', 'auto'], \n","    'shrinking': [True, False], \n","    'probability': [True, False],\n","    'verbose':[True, False]}\n","param={'kernel': ['linear']}\n","pre_model = svm.SVC() #svm paramters tuning\n","model = GridSearchCV(pre_model,\n","                      param_grid=param,\n","                      scoring='accuracy',\n","                      cv=5,\n","                      return_train_score=True)'''\n","model = svm.SVC()\n","##################################################################\n","#################### RF #######################\n","'''param={'n_estimators': [100, 200]}\n","pre_model = RandomForestClassifier()\n","model = GridSearchCV(pre_model,\n","                      param_grid=param,\n","                      scoring='accuracy',\n","                      cv=5,\n","                      return_train_score=True)'''\n","#model = RandomForestClassifier()\n","##################################################################\n","#################### SGD #######################\n","'''pre_model = SGDClassifier()\n","param = dict(\n","    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n","    learning_rate=['optimal', 'invscaling', 'adaptive'],\n","    eta0=uniform(loc=1e-7, scale=1e-2)\n",")\n","print = RandomizedSearchCV(\n","    estimator=pre_model,\n","    param_distributions=param,\n","    cv=5,\n","    n_iter=50\n",")'''\n","#model = SGDClassifier()\n","##################################################################\n","#################### bnb #######################\n","#model = BernoulliNB()\n","'''param={'alpha': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000], 'fit_prior': [True, False]}\n","pre_model = BernoulliNB() #svm paramters tuning\n","model = GridSearchCV(pre_model,\n","                      param_grid=param,\n","                      scoring='accuracy',\n","                      cv=5,\n","                      return_train_score=True)'''\n","##################################################################\n","#################### logistic #######################\n","param={'penalty': ['l1', 'l2', 'elasticnet', 'none'], 'C': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]}\n","pre_model = LogisticRegression() #svm paramters tuning\n","model = GridSearchCV(pre_model,\n","                      param_grid=param,\n","                      scoring='accuracy',\n","                      cv=5,\n","                      return_train_score=True)\n","##################################################################\n","model.fit(X_train, train_token['sentiment'])\n","print('Finish train train data')\n","'''print(model.best_params_)\n","print(model.best_estimator_)\n","print(model.best_score_)'''\n","\n","# save best model\n","with open(filename, 'wb') as file:\n","  joblib.dump(model, file, compress=1)\n","print('Finish save model')"],"metadata":{"id":"Cr3zBPu-52Dn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing"],"metadata":{"id":"npyenjTx6F22"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import multiprocessing\n","from multiprocessing import Pool\n","import scipy.sparse as sp\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import GridSearchCV\n","import joblib\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","\n","filename = 'rf_model.pkl'\n","num_cores = multiprocessing.cpu_count()\n","num_partitions = num_cores-2 # I like to leave some cores for other\n","#processes\n","print(num_partitions)\n","def parallelize_dataframe(df, func):\n","    a = np.array_split(df, num_partitions)\n","    del df\n","    pool = Pool(num_cores)\n","    df = sp.vstack(pool.map(func, a), format='csr')\n","    pool.close()\n","    pool.join()\n","    return df\n","\n","def test_func(data):\n","    tfidf_matrix = vectorizer.transform(data[\"text\"])\n","    return tfidf_matrix\n","\n","# read the preprocess data\n","train_token = pd.read_csv('train_preprocessed.csv')\n","print('Finish reading preprocessed train data')\n","test_token = pd.read_csv('test_preprocessed.csv')\n","test_token = test_token[test_token.sentiment != 2.0]# drop the class 2, since there is no class 2 in the testing dataset, so we won't get class 2 however hard we tried. Drop it to get the better result for test. \n","print('Finish reading preprocessed test data')\n","\n","#vectorize the data\n","with open('vectorizer.pkl', 'rb') as file:\n","  vectorizer = pickle.load(file)\n","X_train = parallelize_dataframe(train_token, test_func)\n","print('Finish vectorize train data')\n","X_test = vectorizer.transform(test_token['text'].values).toarray()\n","print('Finish vectorize test data')\n","\n","#load files\n","model = joblib.load(filename)\n","\n","# print the result\n","print('train')\n","predictions = model.predict(X_train)\n","print(predictions.shape, train_token['sentiment'].shape)\n","print(accuracy_score(train_token['sentiment'], predictions))\n","print(classification_report(train_token['sentiment'],predictions))\n","\n","print('test')\n","predictions = model.predict(X_test)\n","print(accuracy_score(test_token['sentiment'], predictions))\n","print(classification_report(test_token['sentiment'],predictions))"],"metadata":{"id":"SCyCd98y6CYD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Predict our data"],"metadata":{"id":"Bp1FxV_46Ml2"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import multiprocessing\n","from multiprocessing import Pool\n","import scipy.sparse as sp\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import GridSearchCV\n","import joblib\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","import os\n","from pathlib import Path \n","\n","model_type = 'nb'\n","filename = model_type+'_model.pkl'\n","if not os.path.exists(model_type):\n","    Path(model_type).mkdir(parents=True, exist_ok=True)\n","num_cores = multiprocessing.cpu_count()\n","num_partitions = num_cores-2 # I like to leave some cores for other\n","#processes\n","print(num_partitions)\n","def parallelize_dataframe(df, func):\n","    a = np.array_split(df, num_partitions)\n","    del df\n","    pool = Pool(num_cores)\n","    df = sp.vstack(pool.map(func, a), format='csr')\n","    pool.close()\n","    pool.join()\n","    return df\n","\n","def test_func(data):\n","    tfidf_matrix = vectorizer.transform(data[\"tweets_to_token\"])\n","    return tfidf_matrix\n","\n","#vectorize the data\n","with open('vectorizer.pkl', 'rb') as file:\n","    vectorizer = pickle.load(file)\n","#load files\n","model = joblib.load(filename)\n","# read the preprocess data\n","class_lst = ['barbera', 'cabernet', 'chardonnay', 'merlot', 'montepulciano', 'moscato', 'pinot_grigio', 'sangiovese', 'sauvignon', 'syrah', 'ciu_ciu_1', 'ciu_ciu_2', 'ciu_ciu_3', 'ciu_ciu_4']\n","for c in class_lst:\n","    test_token = pd.read_csv('token'+'/'+c+'_token.csv')\n","    #test_token.drop(['id','text','time','user_location','username','replyCount','retweetCount','likeCount','quoteCount','class'],axis=1,inplace=True)\n","    test_token.tweets_to_token=test_token.tweets_to_token.astype(str)\n","    #print('Finish reading preprocessed test data')\n","\n","    X_test = parallelize_dataframe(test_token, test_func)\n","    #print('Finish vectorize test data')\n","\n","    predictions = model.predict(X_test)\n","    df2 = pd.DataFrame(predictions, columns=['prediction'])\n","    df = pd.concat([test_token, df2], axis=1)\n","    df.to_csv(model_type+'/'+c+'_predict.csv', index=False)\n","    #print(df[df.prediction==0])\n","    del df\n","    del df2\n","    del test_token\n","    del X_test\n","    print(c)\n"],"metadata":{"id":"PkZ1ECm16Itb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Do some analyze on our data"],"metadata":{"id":"IsXj3cXG6Wv9"}},{"cell_type":"code","source":["import pandas as pd\n","#pd.set_option(\"display.max_rows\", None)\n","\n","class_lst = ['Barbera', 'Cabernet', 'Chardonnay', 'Merlot', 'Montepulciano', 'Moscato', 'Pinot Grigio', 'Sangiovese', 'Sauvignon', 'Syrah', 'Ciù Ciù', 'CiuCiu', 'Ciu Ciu', 'CiùCiù']\n","\n","df = []\n","for c in class_lst:\n","    df.append(pd.read_csv(c+'.csv', lineterminator='\\n'))\n","\n","res = pd.concat(df,axis=0, ignore_index=True)\n","# counting unique items\n","item_counts = res[\"id\"].value_counts()\n","#print(item_counts.keys())\n","print(item_counts.value_counts()) #103515/625393 #8525\n","#item_counts[:103515].plot(kind='barh')\n","\n","dupli = res[res.duplicated(subset=['id'], keep=False)]\n","dic = {}\n","#n = 0\n","for c in class_lst:\n","    dic[c] = 0\n","for k in item_counts.keys():\n","    sub = dupli[dupli['id']==k]\n","    keyword = sorted(list(sub.groupby(['class']).groups.keys()))\n","    if len(keyword) == 1:\n","        continue\n","    key =' '.join(keyword)\n","    if key not in dic:\n","        dic[key] = 1\n","    else:\n","        dic[key] += 1\n","    '''for i in range(len(sub)):\n","        dic[sub['class'].iloc[i]] += 1'''\n","#print(dupli.iloc[0])\n","#print(res.loc[res[dupli] == True & res['class'] == 'Barbera'])\n","#print(dic)\n","dic_sort = {k: v for k, v in sorted(dic.items(), key=lambda item: item[1])}\n","print(dic_sort)\n","'''{'Barbera': 725, 'Cabernet': 84491, 'Chardonnay': 17786, 'Merlot': 21596,\n"," 'Montepulciano': 691, 'Moscato': 4111, 'Pinot Grigio': 4376, 'Sangiovese': 2474, \n"," 'Sauvignon': 78323, 'Syrah': 6315, 'Ciù Ciù': 323, 'CiuCiu': 40, 'Ciu Ciu': 323, \n"," 'CiùCiù': 40}'''\n","\n"],"metadata":{"id":"bs_h8UKC6VW0"},"execution_count":null,"outputs":[]}]}